{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Data Mining\n",
    "IFN645 - Data Mining Technologies and Applications\n",
    "Due date: 7th April, 2019\n",
    "\n",
    "Before answering any questions we need to import the neccessary libraries and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "df = pd.read_csv('./data.csv')\n",
    "\n",
    "# Toggle whether or not to show output.\n",
    "verbose = False\n",
    "quick_mode = False\n",
    "target_value = \"IsBadBuy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Data Selection and Distribution. \n",
    "#### 1.1 What is the proportion of cars who can be classified as a “kick”?\n",
    "The proportion of *kicks* can be calculated using the function .mean() because the data is stored binary in the column *IsBadBuy*.  12.95% of the cars are *kicks* in the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(verbose):\n",
    "    print(\"Percentage of kicks: \" + str(df[target_value].mean() * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Did you have to fix any data quality problems? Detail them.\n",
    "We found a bunch of data quality problems that we had to take care of.\n",
    "\n",
    "Almost all data was missing on 44 of the rows, so we decided to remove them from the dataset. The percentage of *kicks* remained 12.95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_name = \"VehYear\"\n",
    "df[c_name] = df[c_name].fillna(0).astype(int)\n",
    "if(verbose):\n",
    "    print(df.loc[df[c_name] == 0])\n",
    "df = df[df[c_name] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns required data cleaning. As we did not have a data/domain expert available we had to make some assumptions:\n",
    "- *Manual* and *MANUAL* is assumed to be the same in the column *Transmission*.\n",
    "- All values except 0 and 0.0 is assumed to be 1 in the column *IsOnlineSale*.\n",
    "- *IsOnlineSale* should be represented as a binary value.\n",
    "- *IsBadBuy* should be represented as binary value.\n",
    "- All *MMR...* values should be interpreted as numbers, not strings.\n",
    "- *WheelType* empty cells should be *?*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_name = \"Transmission\"\n",
    "if(verbose):\n",
    "    print(df[c_name].unique())\n",
    "df[c_name] = df[c_name].mask(df[c_name] == \"Manual\", \"MANUAL\")\n",
    "if(verbose):\n",
    "    print(df[c_name].unique())\n",
    "    \n",
    "c_name = \"IsOnlineSale\"\n",
    "if(verbose):\n",
    "    dg = sns.countplot(data=df, x=c_name)\n",
    "    plt.show()\n",
    "\n",
    "df[c_name] = df[c_name].mask(df[c_name] == 0.0, \"0\")\n",
    "df[c_name] = np.where(df[c_name] == \"0\", False, True)\n",
    "\n",
    "if(verbose):\n",
    "    dg = sns.countplot(data=df, x=c_name)\n",
    "    plt.show()\n",
    "    \n",
    "c_name = \"IsBadBuy\"\n",
    "if(verbose):\n",
    "    df[c_name].unique()\n",
    "df[c_name] = df[c_name].astype(bool)\n",
    "if(verbose):\n",
    "    df[c_name].unique()\n",
    "    \n",
    "     \n",
    "for c_name in [\"MMRAcquisitionAuctionAveragePrice\",\n",
    "              \"MMRAcquisitionAuctionCleanPrice\",\n",
    "              \"MMRAcquisitionRetailAveragePrice\",\n",
    "              \"MMRAcquisitonRetailCleanPrice\",\n",
    "              \"MMRCurrentAuctionAveragePrice\",\n",
    "              \"MMRCurrentAuctionCleanPrice\",\n",
    "              \"MMRCurrentRetailAveragePrice\",\n",
    "              \"MMRCurrentRetailCleanPrice\",\n",
    "              \"MMRCurrentRetailRatio\",\n",
    "              \"VehBCost\"]:\n",
    "    df[c_name] = df[c_name].mask(df[c_name] == \"?\", 0)\n",
    "    df[c_name] = df[c_name].mask(df[c_name] == \"#VALUE!\", 0)\n",
    "    df[c_name] = pd.to_numeric(df[c_name])\n",
    "    df[c_name] = df[c_name].mask(df[c_name] == 1, 0)\n",
    "    \n",
    "c_name = \"WheelType\"\n",
    "df[c_name] = df[c_name].fillna(\"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Can you identify any clear patterns by initial exploration of the data using histogram or box plot?\n",
    "The *VehYear* column, the year in which the car was made - does seems to have a influence on the *IsBadBuy* varible.  \n",
    "It is more likely that an older car is a *kick*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.boxplot(x=target_value, y='VehYear', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 What variables did you include in the analysis and what were their roles and measurement level set? Justify your choice.\n",
    "We decided to drop the following columns:\n",
    "- PurchaseID\n",
    "- PurchaseTimestamp\n",
    "- PurchasseData\n",
    "- WheelTypeID\n",
    "- PRIMEUNIT\n",
    "- AUCGUART\n",
    "- ForSale\n",
    "\n",
    "Identification columns is not interesting. Almost all cars are marked as for sale. *PRIMEUNIT* and *AUCGUART* is missing data in most of the data entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PurchaseID','PurchaseTimestamp','PurchaseDate','WheelTypeID','PRIMEUNIT','AUCGUART','ForSale'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 What distribution scheme did you use? What data partitioning allocation did you set? Explain your selection.\n",
    "The distribution between good and bad buys was very skewed, which made our prediction models have a heavy tendency to predict most cars as being good buys. We decided to use undersampling to counter the skewness - so the distribution between good and bay buys were equal in our traning sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Predictive Modeling Using Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# prepare data for decision tree\n",
    "df_decision = pd.get_dummies(df)\n",
    "df_decision = df_decision.dropna()\n",
    "\n",
    "# target/input split\n",
    "y = df_decision[target_value]\n",
    "X = df_decision.drop([target_value],axis = 1)\n",
    "\n",
    "# setting random state\n",
    "rs = 10\n",
    "\n",
    "# undersampling \n",
    "badBuy_indices = df_decision[df_decision[target_value] == 0].index\n",
    "\n",
    "sample_size = sum(df_decision.IsBadBuy == 1)\n",
    "\n",
    "random_indices = np.random.choice(badBuy_indices, sample_size, replace=False)\n",
    "\n",
    "Non_badBuy_sample = df_decision.loc[random_indices]\n",
    "\n",
    "Badbuys = df_decision.loc[df_decision[target_value]==1]\n",
    "\n",
    "sample_data = Badbuys.append(Non_badBuy_sample,ignore_index=True)\n",
    "X = sample_data.drop([target_value],axis = 1)\n",
    "y = sample_data[target_value]\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)\n",
    "\n",
    "#save X_test for later use\n",
    "tree_X_test = X_test\n",
    "tree_y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Build a decision tree using the default setting. \n",
    "**(a) What is the classification accuracy on training and test datasets?**  \n",
    "Full print-out provided below. 77% on training, 66% on test.\n",
    "\n",
    "**(b) What is the size of tree (i.e. number of nodes)?**  \n",
    "Default settings gives a tree with 2743 nodes, tweaked has 53 nodes.\n",
    "\n",
    "**(c) How many leaves are in the tree that is selected based on the validation dataset?**  \n",
    "27 leaves in the tree with a total of 53 nodes.\n",
    "\n",
    "**(d) Which variable is used for the first split? What are the competing splits for this first split?**  \n",
    "Our tree first splits on *WheelType_?*, or in English words - weather the type of wheels on the car is known.\n",
    "Another close contender is *VehYear*, the year the vechicle was manufactured.\n",
    "\n",
    "**(e) What are the 5 important variables in building the tree?**  \n",
    "- WheelType (unknown, boolean value)\n",
    "- VehYear\n",
    "- MMRAcquisitionAuctionAveragePrice\n",
    "- VehBCost\n",
    "- VehOdo\n",
    "\n",
    "**(f) Report if you see any evidence of model overfitting.**  \n",
    "There is signs of overfitting for the default settings and on the tweaked tree.\n",
    "\n",
    "**(g) Did changing the default setting (i.e., only focus on changing the setting of the number of splits to create a node) help improving the model? Answer the above questions on the best performing tree.**  \n",
    "Changing *min_samples_leaf* had a negative effect on the results.  \n",
    "Changing *max_depth* helped us get better result on the test data (minimized overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "if(verbose):\n",
    "    print(\"DEFAULT SETTINGS:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Number of nodes: \" + str(model.tree_.node_count))\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=5,random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "if(verbose):\n",
    "    print(\"\\nTWEAKED SETTINGS:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Number of nodes: \" + str(model.tree_.node_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Build another decision tree tuned with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(quick_mode):\n",
    "    print(\"Turn off quick mode to enable gridsearch.\")\n",
    "else:\n",
    "    params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 10),\n",
    "          'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "    cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "    cv.fit(X_train, y_train)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "        print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "    y_pred = cv.predict(X_test)\n",
    "    \n",
    "    # Use this decision tree model\n",
    "    model_dt = cv\n",
    "    dt_model = cv.best_estimator_\n",
    "    ypred_dt = model_dt.predict_proba(X_test)\n",
    "    report_dt = classification_report(y_test, y_pred)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(report_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) What is the classification accuracy on training and test datasets?**  \n",
    "The result is 71% on the training data and 66% on the test data. The full report is printed above.  \n",
    "\n",
    "**(b) What is the size of tree (i.e. number of nodes)? Is the size different from the maximal tree or the tree in the previous step? Why?**  \n",
    "It is way lower than the maximal tree - a maximal tree would not be very useful, it would be overfitting a lot. Also, our max_depth does not allow for the creation of a maximal tree. The previous tree contained 53 nodes and the tuned tree also contains 53 nodes.\n",
    "\n",
    "**(c) How many leaves are in the tree that is selected based on the validation data set?**  \n",
    "This tree contains 27 leaves.\n",
    "\n",
    "**(d) Which variable is used for the first split? What are the competing splits for this first split?**  \n",
    "Same as previous tree, the type of the wheel.\n",
    "\n",
    "**(e) What are the 5 important variables in building the tree?**\n",
    "- WheelType (unknown, boolean value)\n",
    "- VehYear\n",
    "- MMRAcquisitionAuctionAveragePrice\n",
    "- VehBCost\n",
    "- VehOdo\n",
    "\n",
    "**(f) Report if you see any evidence of model overfitting.**   \n",
    "There is a slight overfitting.\n",
    "\n",
    "**(g) What are the parameters used? Explain your choices.**  \n",
    "We tried to test both the *gini* and *entropy* way of measuring quality of the split.  \n",
    "*Max_depth* is varied to try to improve accuracy without overfitting the model and the *min_samples_leaf* to test if internal node splitting would result in a different and more accurate tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 What is the significant difference do you see between these two decision tree models?\n",
    "*How do they compare performance-wise? Explain why those changes may have happened.*  \n",
    "The performance change is insignificant. The two decision trees are almost the same.  \n",
    "This is probably caused by the most important configuration in this case being *max_depth* which we adjusted before running grid search to find parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 From the better model, can you identify which cars could potential be *kicks*?\n",
    "Cars where the wheel type is known and from the MANHEIM auction are likely to be good buys. \n",
    "Cars where the wheel type is unknown, older then 2005 and a cost of less than 6075$ are likely to be bad buys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Predictive Modeling Using Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10602 entries, 0 to 10601\n",
      "Columns: 127 entries, VehYear to VNST_WV\n",
      "dtypes: bool(2), float64(11), int32(1), int64(1), uint8(112)\n",
      "memory usage: 2.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "sample_data.info()\n",
    "\n",
    "\n",
    "X = sample_data.drop([target_value],axis = 1)\n",
    "y = sample_data[target_value]\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 In preparation for regression, is any imputation of missing values needed for this data set? List the variables that did.\n",
    "> 509 cars need *MMR___* data  \n",
    "> 44 cars have already been removed due to missing information (year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Apply transformation method(s) to the variable(s) that need it. List the variables that needed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_skewed_columns(df):\n",
    "    # setting up subplots for easier visualisation\n",
    "    f, axes = plt.subplots(4,3, figsize=(10,10), sharex=False)\n",
    "\n",
    "    sns.distplot(df['MMRAcquisitionAuctionAveragePrice'].dropna(), hist=False, ax=axes[0,0])\n",
    "    sns.distplot(df['MMRAcquisitionAuctionCleanPrice'].dropna(), hist=False, ax=axes[0,1])\n",
    "    sns.distplot(df['MMRAcquisitionRetailAveragePrice'].dropna(), hist=False, ax=axes[0,2])\n",
    "    sns.distplot(df['MMRAcquisitonRetailCleanPrice'].dropna(), hist=False, ax=axes[1,0])\n",
    "    sns.distplot(df['MMRCurrentAuctionAveragePrice'].dropna(), hist=False, ax=axes[1,1])\n",
    "    sns.distplot(df['MMRCurrentAuctionCleanPrice'].dropna(), hist=False, ax=axes[1,2])\n",
    "    sns.distplot(df['MMRCurrentRetailAveragePrice'].dropna(), hist=False, ax=axes[2,0])\n",
    "    sns.distplot(df['MMRCurrentRetailCleanPrice'].dropna(), hist=False, ax=axes[2,1])\n",
    "    sns.distplot(df['MMRCurrentRetailRatio'].dropna(), hist=False, ax=axes[2,2])\n",
    "    sns.distplot(df['VehBCost'].dropna(), hist=False, ax=axes[3,0])\n",
    "    plt.show()\n",
    "    \n",
    "if(verbose):\n",
    "    plot_skewed_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list columns to be transformed\n",
    "columns_to_transform = ['MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice', 'MMRAcquisitionRetailAveragePrice',\n",
    "                        'MMRAcquisitonRetailCleanPrice','MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice',\n",
    "                        'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice','MMRCurrentRetailRatio','VehBCost']\n",
    "\n",
    "# copy the dataframe\n",
    "df_log = sample_data.copy()\n",
    "\n",
    "# transform the columns with np.log\n",
    "for col in columns_to_transform:\n",
    "    df_log[col] = df_log[col].apply(lambda x: x+1)\n",
    "    df_log[col] = df_log[col].apply(np.log)\n",
    "\n",
    "# plot them again to show the distribution\n",
    "if(verbose):\n",
    "    plot_skewed_columns(df_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "y_log = df_log[target_value]\n",
    "X_log = df_log.drop([target_value], axis=1)\n",
    "\n",
    "X_mat_log = X_log.as_matrix()\n",
    "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_mat_log, y_log, test_size=0.3, stratify=y_log, \n",
    "                                                                    random_state=rs)\n",
    "\n",
    "# standardise them\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log = scaler_log.fit_transform(X_train_log, y_train_log)\n",
    "X_test_log = scaler_log.transform(X_test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Build a regression model using the default regression method with all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.697749629429996\n",
      "Test accuracy: 0.68531908204967\n"
     ]
    }
   ],
   "source": [
    "# fit it to training data\n",
    "model = LogisticRegression(random_state=rs)\n",
    "model.fit(X_train_log, y_train_log)\n",
    "\n",
    "# classification report on test data\n",
    "print(\"Train accuracy:\", model.score(X_train_log, y_train_log))\n",
    "print(\"Test accuracy:\", model.score(X_test_log, y_test_log))\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test_log)\n",
    "# Use this regression model\n",
    "model_reg = model\n",
    "ypred_reg = model_reg.predict_proba(X_test)\n",
    "report_reg = classification_report(y_test, y_pred)\n",
    "\n",
    "if(verbose):\n",
    "    print(report_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 ... Once you done it, build another one and tune it using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6966716076000539\n",
      "Test accuracy: 0.6828041496384785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.78      0.71      1590\n",
      "        True       0.73      0.59      0.65      1591\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      3181\n",
      "   macro avg       0.69      0.68      0.68      3181\n",
      "weighted avg       0.69      0.68      0.68      3181\n",
      "\n",
      "{'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_log, y_train_log)\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", cv.score(X_train_log, y_train_log))\n",
    "print(\"Test accuracy:\", cv.score(X_test_log, y_test_log))\n",
    "\n",
    "y_pred = cv.predict(X_test_log)\n",
    "print(classification_report(y_test_log, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Name the regression function used.**  \n",
    "Logistic regression was used to build the model.\n",
    "\n",
    "**(i) How much was the difference in performance of two models build, default and optimal?**  \n",
    "We can tell from the result above. The GridSearchCV did not make any change to the result, and the parameter C is 1.\n",
    "\n",
    "**(j) Show the set parameters for the best model. What are the parameters used?\n",
    "Explain your decision. What are the optimal parameters?**  \n",
    "The set parameters is based on GridSearchCV which tries different parameters from a typical range of values and pick the optimal ones. In sklearn logistic regression, regularisation is implemented in the hyperparameter C, which denotes the inverse of regularisation strength. Smaller C means stronger regularisation. The C for our best model is 1.\n",
    "\n",
    "**(k) Report which variables are included in the regression model.**  \n",
    "As we only droped 'PurchaseID','PurchaseTimestamp','PurchaseDate','WheelTypeID','PRIMEUNIT','AUCGUART','ForSale', all remained variables are all prossessed in the regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10602 entries, 0 to 10601\n",
      "Columns: 127 entries, VehYear to VNST_WV\n",
      "dtypes: bool(2), float64(11), int32(1), int64(1), uint8(112)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "sample_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = model.coef_[0]\n",
    "feature_names = X_log.columns\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "indices = indices[:5]\n",
    "if(verbose):\n",
    "    for i in indices:\n",
    "        print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(l) Report the top-5 important variables (in the order) in the model.**  \n",
    "- MMRAcquisitionAuctionAveragePrice \n",
    "- MMRAcquisitionRetailAveragePrice  \n",
    "- WheelType_? (it means unknown or not)\n",
    "- MMRCurrentAuctionAveragePrice   \n",
    "- MMRCurrentRetailCleanPrice   \n",
    "\n",
    "**(m) What is classification accuracy on training and test datasets?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.697749629429996\n",
      "Test accuracy: 0.68531908204967\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.78      0.71      1590\n",
      "        True       0.73      0.59      0.65      1591\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      3181\n",
      "   macro avg       0.69      0.68      0.68      3181\n",
      "weighted avg       0.69      0.68      0.68      3181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", model.score(X_train_log, y_train_log))\n",
    "print(\"Test accuracy:\", model.score(X_test_log, y_test_log))\n",
    "print(classification_report(y_test_log, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(n) Report any sign of overfitting**\n",
    "\n",
    "We can see the test accuracy is lower than train accuracy due to a slight overfitting.\n",
    "\n",
    "#### 3.4 Build another regression model using the subset of inputs selected by RFE and selection by model method. Answer the followings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.694919822126398\n",
      "Test accuracy: 0.6862621817038667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.79      0.72      1590\n",
      "        True       0.73      0.58      0.65      1591\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      3181\n",
      "   macro avg       0.69      0.69      0.68      3181\n",
      "weighted avg       0.69      0.69      0.68      3181\n",
      "\n",
      "{'C': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfe = RFECV(estimator = LogisticRegression(random_state=rs,solver = 'liblinear'), cv=10)\n",
    "rfe.fit(X_train_log, y_train_log) # run the RFECV\n",
    "\n",
    "\n",
    "\n",
    "X_train_sel = rfe.transform(X_train_log)\n",
    "X_test_sel = rfe.transform(X_test_log)\n",
    "\n",
    "#save X_test_sel for later use\n",
    "reg_X_test = X_test_sel\n",
    "reg_y_test = y_test_log\n",
    "\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_sel, y_train_log)\n",
    "log_reg_model = cv.best_estimator_\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", cv.score(X_train_sel, y_train_log))\n",
    "print(\"Test accuracy:\", cv.score(X_test_sel, y_test_log))\n",
    "\n",
    "y_pred = cv.predict(X_test_sel)\n",
    "print(classification_report(y_test_log, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature set 126\n",
      "Number of features after elimination 37\n"
     ]
    }
   ],
   "source": [
    "print(\"Original feature set\", X_train_log.shape[1])\n",
    "print(\"Number of features after elimination\", rfe.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Report which variables are included in the regression model.** \n",
    "There are 69 variables are selected by RFE to process in the regression model.\n",
    "\n",
    "**(b) Report the top-5 important variables (in the order) in the model.**  \n",
    "WheelType_?\n",
    "MMRAcquisitionAuctionAveragePrice\n",
    "MMRCurrentAuctionAveragePrice\n",
    "MMRAcquisitionRetailAveragePrice\n",
    "MMRCurrentRetailCleanPrice\n",
    "the top-5 impoortant variables remain the same as only non-improtant variables are affected by RFE.\n",
    "\n",
    "**(c). What are the parameters used? Explain your choices. What are the optimal parameters? Which regression function is being used?**  \n",
    "Parameter C is used, which denotes the inverse of regularisation strength. cv is given as 10. Other parameters are all default values. \n",
    "\n",
    "**(d). Report any sign of overfitting.**  \n",
    "Train accuracy: 0.616763239455599  \n",
    "Test accuracy: 0.6101854762653254  \n",
    "The train accuracy is very close to the test accuracy, it indicates there is no obivious overfitting.\n",
    "\n",
    "**(e). What is classification accuracy on training and test datasets?**  \n",
    "Train accuracy: 0.6939765530251988  \n",
    "Test accuracy: 0.6928638792832442\n",
    "\n",
    "**(f). Did it improve/worsen the performance? Explain why those changes may have happened.**  \n",
    "The performance is almost the same. Test accuracy is from 0.6828041496384785 to 0.6831185161898774. The non-important variables do not make much contribution to the result, so it does not make much difference whether we elinimate them or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Using the best regression model, which cars could potential be “kicks”? Can you provide some descriptive summary of those cars?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the regression model from its importance list. If a car has unknown WheelType, or the AuctionAveragePrice is quite low,it has higher possibility to be a kick. MMRAcquisitionRetailAveragePrice and MMRCurrentAuctionAveragePrice have postive influence on cars' reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Predictive Modeling Using Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# data prep\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "df.drop(['PurchaseID','PurchaseTimestamp',\n",
    "        'PurchaseDate','PRIMEUNIT','AUCGUART','ForSale','Color',\n",
    "        'WheelType','Nationality','TopThreeAmericanName','WarrantyCost'],axis=1,inplace=True)\n",
    "\n",
    "c_name = \"VehYear\"\n",
    "df[c_name] = df[c_name].fillna(0).astype(int)\n",
    "df = df[df[c_name] != 0]\n",
    "    \n",
    "c_name = \"Transmission\"\n",
    "df[c_name] = df[c_name].mask(df[c_name] == \"Manual\", \"MANUAL\")\n",
    "df[c_name] = df[c_name].mask(df[c_name] == \"?\", \"AUTO\")\n",
    "    \n",
    "c_name = \"WheelTypeID\"\n",
    "df[c_name] = df[c_name].mask(df[c_name] == \"0\", \"?\")\n",
    "df[c_name] = df[c_name].fillna(\"?\")\n",
    "\n",
    "c_name = \"VehBCost\"\n",
    "df[c_name] = df[c_name].mask(df[c_name] == \"?\", \"0\")\n",
    "df[c_name] = pd.to_numeric(df[c_name])\n",
    "df[c_name] = df[c_name].mask(df[c_name] < 0, 0)\n",
    "\n",
    "c_name = \"IsOnlineSale\"\n",
    "df[c_name] = np.where(df[c_name] == \"0\", False, True)\n",
    "    \n",
    "c_name = \"IsBadBuy\"\n",
    "df[c_name] = df[c_name].astype(bool)\n",
    "    \n",
    "collection = [\"MMRAcquisitionAuctionAveragePrice\",\n",
    "              \"MMRAcquisitionAuctionCleanPrice\",\n",
    "              \"MMRAcquisitionRetailAveragePrice\",\n",
    "              \"MMRAcquisitonRetailCleanPrice\",\n",
    "              \"MMRCurrentAuctionAveragePrice\",\n",
    "              \"MMRCurrentAuctionCleanPrice\",\n",
    "              \"MMRCurrentRetailAveragePrice\",\n",
    "              \"MMRCurrentRetailCleanPrice\",\"MMRCurrentRetailRatio\"]\n",
    "\n",
    "for c_name in collection:\n",
    "    df[c_name] = df[c_name].mask(df[c_name] == \"?\", 0)\n",
    "    df[c_name] = df[c_name].mask(df[c_name] == \"#VALUE!\", 0)\n",
    "    df[c_name] = pd.to_numeric(df[c_name])\n",
    "    df[c_name] = df[c_name].mask(df[c_name] == 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "df_reg = pd.get_dummies(df)\n",
    "df_reg = df_reg.dropna()\n",
    "\n",
    "# undersampling \n",
    "badBuy_indices = df_reg[df_reg[target_value] == 0].index\n",
    "\n",
    "sample_size = sum(df_reg.IsBadBuy == 1)\n",
    "\n",
    "random_indices = np.random.choice(badBuy_indices, sample_size, replace=False)\n",
    "\n",
    "Non_badBuy_sample = df_reg.loc[random_indices]\n",
    "\n",
    "Badbuys = df_reg.loc[df_reg[target_value]==1]\n",
    "\n",
    "sample_data = Badbuys.append(Non_badBuy_sample,ignore_index=True)\n",
    "X = sample_data.drop([target_value],axis = 1)\n",
    "y = sample_data[target_value]\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)\n",
    "\n",
    "# train test split\n",
    "y = sample_data['IsBadBuy']\n",
    "X = sample_data.drop(['IsBadBuy'], axis=1)\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8876162242285406\n",
      "Test accuracy: 0.6463376296762025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.65      0.65      1590\n",
      "        True       0.65      0.65      0.65      1591\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      3181\n",
      "   macro avg       0.65      0.65      0.65      3181\n",
      "weighted avg       0.65      0.65      0.65      3181\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=10, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# default neural network model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) What is the network architecture?**  \n",
    "The hidden layer sizes is 100.\n",
    "\n",
    "**(b) How many iterations are needed to train this network?**   \n",
    "More than 200 as it warned max iterations reached.\n",
    "\n",
    "**(c) Do you see any sign of over-fitting?**  \n",
    "Yes, as the train accuracy is much higher than test accuracy.\n",
    "\n",
    "**(d) Did the training process converge and resulted in the best model?**  \n",
    "No, the training process did not converge and it is not the best model.\n",
    "\n",
    "**(e) What is classification accuracy on training and test datasets?**  \n",
    "Train accuracy: 0.8974531734267619  \n",
    "Test accuracy: 0.6441370638164099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7421, 96)\n"
     ]
    }
   ],
   "source": [
    "# See how many input features we have by printing out the train shape.\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7214661096887212\n",
      "Test accuracy: 0.6664570889657341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.72      0.68      1590\n",
      "        True       0.69      0.62      0.65      1591\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      3181\n",
      "   macro avg       0.67      0.67      0.67      3181\n",
      "weighted avg       0.67      0.67      0.67      3181\n",
      "\n",
      "{'hidden_layer_sizes': (5,)}\n"
     ]
    }
   ],
   "source": [
    "# With 96 features, we will start tuning with one hidden layer of 5 to 96 neurons, increment of 15. \n",
    "params = {'hidden_layer_sizes': [(x,) for x in range(5, 96, 15)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7214661096887212\n",
      "Test accuracy: 0.6664570889657341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.72      0.68      1590\n",
      "        True       0.69      0.62      0.65      1591\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      3181\n",
      "   macro avg       0.67      0.67      0.67      3181\n",
      "weighted avg       0.67      0.67      0.67      3181\n",
      "\n",
      "{'hidden_layer_sizes': (5,)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# new parameters\n",
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7249696806360328\n",
      "Test accuracy: 0.6623703237975479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.70      0.67      1590\n",
      "        True       0.68      0.62      0.65      1591\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      3181\n",
      "   macro avg       0.66      0.66      0.66      3181\n",
      "weighted avg       0.66      0.66      0.66      3181\n",
      "\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (5,)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) What is the network architecture?**  \n",
    "The hidden layer sizes is 5 on alpha 0.001.\n",
    "\n",
    "**(b) How many iterations are needed to train this network?**  \n",
    "The training process converged on default max iterations of 200, thus it is lower than 200.\n",
    "\n",
    "**(c) Do you see any sign of over-fitting?**  \n",
    "Yes, as the train accuracy is a little higher than test accuracy.\n",
    "\n",
    "**(d) Did the training process converge and resulted in the best model?**  \n",
    "Yes, the training process converge and it is the best model.\n",
    "\n",
    "**(e) What is classification accuracy on training and test datasets?**  \n",
    "Train accuracy: 0.7253739388222612  \n",
    "Test accuracy: 0.6645708896573405"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Would feature selection help here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Build another Neural Network model with inputs selected from RFE with regression (use the best model generated in Task 3) and selection with decision tree (use the best model from Task 2).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7097426222881014\n",
      "Test accuracy: 0.6752593524049041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.78      0.70      1590\n",
      "        True       0.72      0.57      0.64      1591\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      3181\n",
      "   macro avg       0.68      0.68      0.67      3181\n",
      "weighted avg       0.68      0.68      0.67      3181\n",
      "\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (3,)}\n"
     ]
    }
   ],
   "source": [
    "# use the transformed data\n",
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_log, y_train_log)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_log, y_train_log))\n",
    "print(\"Test accuracy:\", cv.score(X_test_log, y_test_log))\n",
    "\n",
    "y_pred = cv.predict(X_test_log)\n",
    "print(classification_report(y_test_log, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rfe = RFECV(estimator = LogisticRegression(random_state=rs, solver = 'liblinear'), cv=10)\n",
    "rfe.fit(X_train_log, y_train_log)\n",
    "\n",
    "print(rfe.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7023312222072498\n",
      "Test accuracy: 0.6834328827412763\n"
     ]
    }
   ],
   "source": [
    "# transform log \n",
    "X_train_rfe = rfe.transform(X_train_log)\n",
    "X_test_rfe = rfe.transform(X_test_log)\n",
    "\n",
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_rfe, y_train_log)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_rfe, y_train_log))\n",
    "print(\"Test accuracy:\", cv.score(X_test_rfe, y_test_log))\n",
    "\n",
    "y_pred = cv.predict(X_test_rfe)\n",
    "\n",
    "# Use this neural network model\n",
    "model_neural = cv\n",
    "ypred_neural = model_neural.predict_proba(X_test_rfe)\n",
    "report_neural = classification_report(y_test, y_pred)\n",
    "\n",
    "if(verbose):\n",
    "    print(report_neural)\n",
    "    print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=10,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'max_depth': range(2, 10), 'min_samples_leaf': range(20, 60, 10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': range(2, 10),\n",
    "        'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7421, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selectmodel = SelectFromModel(cv.best_estimator_, prefit=True)\n",
    "X_train_sel_model = selectmodel.transform(X_train)\n",
    "X_test_sel_model = selectmodel.transform(X_test)\n",
    "\n",
    "print(X_train_sel_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6918205093653147\n",
      "Test accuracy: 0.6746306193021062\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.75      0.70      1590\n",
      "        True       0.70      0.60      0.65      1591\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      3181\n",
      "   macro avg       0.68      0.67      0.67      3181\n",
      "weighted avg       0.68      0.67      0.67      3181\n",
      "\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (7,)}\n"
     ]
    }
   ],
   "source": [
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_sel_model, y_train)\n",
    "nn_model = cv.best_estimator_\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_sel_model, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test_sel_model, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test_sel_model)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Did feature selection help here? Any change in the network architecture? What inputs are being used as the network input?**\n",
    "\n",
    "Had some help as it slight increased the accuracy. The inputs are from the best model in logical regression and decision tree.\n",
    "\n",
    "**(b) What is classification accuracy on training and test datasets? Is there any improvement in the outcome?**\n",
    "\n",
    "Using decision tree inputs:\n",
    "Train accuracy: 0.6853523783856623  \n",
    "Test accuracy: 0.6736875196479095\n",
    "\n",
    "Using regression inputs:\n",
    "Train accuracy: 0.7110901495755289  \n",
    "Test accuracy: 0.6677145551713297\n",
    "\n",
    "Both have a similar slight improvement in the outcome.\n",
    "\n",
    "**(c) How many iterations are now needed to train this network?**\n",
    "\n",
    "Less than the default 200, as there is no warning.\n",
    "\n",
    "**(d) Do you see any sign of over-fitting?**\n",
    "\n",
    "Yes, both have slightly over-fitting as train accuracy is higher than test accuracy.\n",
    "\n",
    "**(e) Did the training process converge and resulted in the best model?**\n",
    "\n",
    "Yes, the training process converged and resulted in the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the comparison methods, which of the models (i.e one with selected variables and another with all variables) appears to be better?**\n",
    "\n",
    "The model with selected variables of decision tree best model appears to be better as it has relatively higer test accuracy.\n",
    "\n",
    "**From the better model, can you identify cars those could potential be “kicks”? Can you provide some descriptive summary of those cars?**\n",
    "\n",
    "No, neural networks did not provide importance of each attributes.\n",
    "\n",
    "**Is it easy to comprehend the performance of the best neural network model for decision making?**\n",
    "\n",
    "No, the neural network model return outputs according to the inputs and it does not explain how it make the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Generating an Ensemble Model and Comparing Models\n",
    "#### 5.1 Generate an ensemble model to include the best regression model, best decision tree model, and best neural network model. Does the Ensemble model outperform the underlying models? Resonate your answer.\n",
    "\n",
    "The test accuracy and ROC score are higher than all the three individual models, thus the ensemble modal outperform the underlyiing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble train accuracy: 0.7261824551947177\n",
      "Ensemble test accuracy: 0.6765168186104998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.66      0.75      0.70      1590\n",
      "        True       0.70      0.61      0.65      1591\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      3181\n",
      "   macro avg       0.68      0.68      0.67      3181\n",
      "weighted avg       0.68      0.68      0.67      3181\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Schmidt\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import the model\n",
    "voting = VotingClassifier(estimators=[('dt', model_dt), ('lr', model_reg), ('nn', model_neural)], voting='soft')\n",
    "\n",
    "# fit the voting classifier to training data\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "# evaluate train and test accuracy\n",
    "print(\"Ensemble train accuracy:\", voting.score(X_train, y_train))\n",
    "print(\"Ensemble test accuracy:\", voting.score(X_test, y_test))\n",
    "\n",
    "# evaluate ROC auc score\n",
    "y_pred_proba_ensemble = voting.predict_proba(X_test)\n",
    "roc_index_ensemble = roc_auc_score(y_test, y_pred_proba_ensemble[:, 1])\n",
    "\n",
    "y_pred = voting.predict(X_test)\n",
    "report_ens = classification_report(y_test, y_pred)\n",
    "\n",
    "print(report_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Use the comparison methods (or the comparison node) to compare the best decision tree model, the best regression model, the best neural network model and the ensemble model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on test for DT: 0.6614272241433512\n",
      "Accuracy score on test for logistic regression: 0.6862621817038667\n",
      "Accuracy score on test for NN: 0.6746306193021062\n"
     ]
    }
   ],
   "source": [
    "y_pred_dt = dt_model.predict(tree_X_test)\n",
    "y_pred_log_reg = log_reg_model.predict(reg_X_test)\n",
    "y_pred_nn = nn_model.predict(X_test_sel_model)\n",
    "\n",
    "print(\"Accuracy score on test for DT:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Accuracy score on test for logistic regression:\", accuracy_score(y_test, y_pred_log_reg))\n",
    "print(\"Accuracy score on test for NN:\", accuracy_score(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical prediction\n",
    "y_pred = dt_model.predict(tree_X_test)\n",
    "\n",
    "# probability prediction from decision tree\n",
    "y_pred_proba_dt = dt_model.predict_proba(tree_X_test)\n",
    "\n",
    "if(verbose):\n",
    "    print(\"Probability produced by decision tree for each class vs actual prediction on TargetB (0 = non-donor, 1 = donor). You should be able to see the default threshold of 0.5.\")\n",
    "    print(\"(Probs on zero)\\t(probs on one)\\t(prediction made)\")\n",
    "\n",
    "if(verbose):\n",
    "    for i in range(20):\n",
    "        print(y_pred_proba_dt[i][0], '\\t', y_pred_proba_dt[i][1], '\\t', y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC index on test for DT: 0.7334042906443082\n",
      "ROC index on test for logistic regression: 0.7596116520206034\n",
      "ROC index on test for NN: 0.7434472208057114\n",
      "ROC index on test for ensemble model: 0.753180429222553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_proba_dt = dt_model.predict_proba(tree_X_test)\n",
    "y_pred_proba_log_reg = log_reg_model.predict_proba(reg_X_test)\n",
    "y_pred_proba_nn = nn_model.predict_proba(X_test_sel_model)\n",
    "\n",
    "roc_index_dt = roc_auc_score(tree_y_test, y_pred_proba_dt[:,1])\n",
    "roc_index_log_reg = roc_auc_score(reg_y_test, y_pred_proba_log_reg[:,1])\n",
    "roc_index_nn = roc_auc_score(y_test, y_pred_proba_nn[:,1])\n",
    "\n",
    "print(\"ROC index on test for DT:\", roc_index_dt)\n",
    "print(\"ROC index on test for logistic regression:\", roc_index_log_reg)\n",
    "print(\"ROC index on test for NN:\", roc_index_nn)\n",
    "print(\"ROC index on test for ensemble model:\", roc_index_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_pred_proba_dt[:,1])\n",
    "fpr_log_reg, tpr_log_reg, thresholds_log_reg = roc_curve(y_test, y_pred_proba_log_reg[:,1])\n",
    "fpr_nn, tpr_nn, thresholds_nn = roc_curve(y_test, y_pred_proba_nn[:,1])\n",
    "fpr_ens, tpr_ens, thresholds_ens = roc_curve(y_test, y_pred_proba_ensemble[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.64      0.75      0.69      1590\n",
      "        True       0.69      0.58      0.63      1591\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      3181\n",
      "   macro avg       0.67      0.66      0.66      3181\n",
      "weighted avg       0.67      0.66      0.66      3181\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.66      0.78      0.71      1590\n",
      "        True       0.73      0.59      0.65      1591\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      3181\n",
      "   macro avg       0.69      0.69      0.68      3181\n",
      "weighted avg       0.69      0.69      0.68      3181\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.66      0.76      0.71      1590\n",
      "        True       0.72      0.61      0.66      1591\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      3181\n",
      "   macro avg       0.69      0.68      0.68      3181\n",
      "weighted avg       0.69      0.68      0.68      3181\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.66      0.75      0.70      1590\n",
      "        True       0.70      0.61      0.65      1591\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      3181\n",
      "   macro avg       0.68      0.68      0.67      3181\n",
      "weighted avg       0.68      0.68      0.67      3181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ypred_voting = voting.predict_proba(X_test)\n",
    "\n",
    "roc_index_dt = roc_auc_score(y_test, ypred_dt[:, 1])\n",
    "roc_index_reg = roc_auc_score(y_test, ypred_reg[:, 1])\n",
    "roc_index_nn = roc_auc_score(y_test, ypred_neural[:, 1])\n",
    "roc_index_vote = roc_auc_score(y_test, y_pred_proba_ensemble[:,1])\n",
    "\n",
    "plt.plot(fpr_dt, tpr_dt, label='Decision Tree'.format(roc_index_dt), color='red', lw=0.5)\n",
    "plt.plot(fpr_log_reg, tpr_log_reg, label='Regression'.format(roc_index_reg), color='green', lw=0.5)\n",
    "plt.plot(fpr_nn, tpr_nn, label='Neural Network'.format(roc_index_nn), color='darkorange', lw=0.5)\n",
    "plt.plot(fpr_ens, tpr_ens, label='Voting'.format(roc_index_vote), color='blue', lw=0.5)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=0.5, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(report_dt)\n",
    "print(report_reg)\n",
    "print(report_neural)\n",
    "print(report_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) ROC Chart (and Index)**  \n",
    "The chart and index shows that ensemble mode has the relative best result, as its index is the higherst in the four models. The second is Neural Network model, then Logical Regression, then decision tree.\n",
    "\n",
    "**(b) Score Ranking (or Accuracy Score)**  \n",
    "By comparing the accuracy score, ensemble mode still has the best result, the second is Neural Networ, then decision tree, then Logical Regression.\n",
    "\n",
    "**(c) Classification report**  \n",
    "The overall result and f1-score shows the ensemble mode has best performance and the other three results are similar.\n",
    "\n",
    "**(d) Output**  \n",
    "This illustrates the ensemble model has the best performance\n",
    "\n",
    "**(5.2b) Do all the models agree on the cars characteristics? How do they vary?**\n",
    "Here, we can see the curve for different models. Ensemble Model(voting one) shows the largest curve area compared to the other three models. Thus, all three statistics that we used collectively agreed on Ensemble Model being the best performing model overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - Final Remarks: Decision Making\n",
    "#### 6.1 Finally, based on all models and analysis, is there a particular model you will use in decision making? Justify your choice.\n",
    "The decision tree might be a better solution for the dealership, even though it does not have the best precision. All the solutions are close in terms of accuracary, and the decision model allow the dealership to use the learning outcome as 'rule of thumb'-tips to improve decision making when making deals. The ensemble model does have a slightly higher accuracy but would recure salesmen to have access to a computer during negotiation and is not precise enough to automate the decision making process.\n",
    "\n",
    "#### 6.2 Can you summarise positives and negatives of each predictive modelling method based on this analysis?\n",
    "Regression and decision tree seems to have similar results - the decision tree is however easier to interpret. The neural network had a strong tendency of overfitting and the resulting model was hard to interprete. We believe it would be a better fit for automated tasks where a higher precision can be achieved. A car deal is a longer process and the price is negotiable, so it is important that the 'learning outcomes' of the data mining can be easily conveyed.\n",
    "\n",
    "#### 6.3 How the outcome of this study can be used by decision makers? \n",
    "The outcome can be used as 'rule of thumb' tips and to highlight 'warning signs' when negotiating the price of a car at the dealership. The precision of the models are not high enough to be blindly trusted as automated decision making."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
